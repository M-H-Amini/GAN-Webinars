{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCGAN-MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMx0R0yf3r5grk5gBsRIcwI",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/M-H-Amini/GAN-Webinars/blob/main/DCGAN_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yUJTjI6J4k6"
      },
      "source": [
        "#  In The Name Of ALLAH\n",
        "#  Generative Adversarial Networks\n",
        "#  PythonChallenge.ir\n",
        "#  Mohammad Hossein Amini (mhamini@aut.ac.ir)\n",
        "#  Lecture 1 - DCGAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1Ig-6fhKTxR"
      },
      "source": [
        "In this lecture we'll be implementing a DCGAN. The theoretical stuff has been discussed in the video. Let's see how things work in practice!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLtuDessS6Q5"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.layers import Dense, Flatten, Reshape, Conv2D, Conv2DTranspose, BatchNormalization, Activation, LeakyReLU\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import BinaryCrossentropy\n",
        "from keras.models import Sequential\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmJUJvyYK7Pz"
      },
      "source": [
        "#  Dataset Preparation\n",
        "We would use the famous **MNIST** dataset for handwritten digits. Let's assume we just want to generate a specific digit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpxssF_sTImR"
      },
      "source": [
        "(Xtrain, ytrain), (_, _) = mnist.load_data()\n",
        "Xtrain = (np.expand_dims(Xtrain[ytrain==8], 3) - 127.5) / 127.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19RV9RRALeir"
      },
      "source": [
        "#  Visualization\n",
        "In order to visualize results, we would implement ```show``` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hvv-A31gtU7b"
      },
      "source": [
        "def show(X, r=4, c=4):\n",
        "  fig, ax = plt.subplots(r, c, True, True)\n",
        "  for i in range(r):\n",
        "    for j in range(c):\n",
        "      ax[i][j].imshow(X[i*c + j, :, :, 0])\n",
        "  plt.show()\n",
        "\n",
        "show(Xtrain, 2, 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "we6DsTrHeKon"
      },
      "source": [
        "We determine image shapes and the our noise dimension."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSjUIdG6Thp-"
      },
      "source": [
        "z_dim = 100\n",
        "img_shape = 28, 28"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhGBhoQ2eTsJ"
      },
      "source": [
        "#  Discriminator\n",
        "Let's build **discriminator** now. We use an MLP as the discriminator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkH9FJKmTb-N"
      },
      "source": [
        "def buildDisc(img_shape=(28, 28)):\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(64, 3, 2, 'same', input_shape=(*img_shape, 1)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(LeakyReLU())\n",
        "  model.add(Conv2D(128, 3, 2, 'same'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(LeakyReLU())\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(1))\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0r6MdziNlaml"
      },
      "source": [
        "# Generator\n",
        "Again, we use an MLP for the generator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irN6OU3ZineQ"
      },
      "source": [
        "def buildGen(z_dim=100, img_shape=(28, 28)):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(7 * 7 * 128, input_shape=(z_dim,)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(LeakyReLU())\n",
        "  model.add(Reshape((7, 7, 128)))\n",
        "  model.add(Conv2DTranspose(64, (5, 5), 2, padding='same'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(LeakyReLU())\n",
        "  model.add(Conv2DTranspose(1, (5, 5), 2, padding='same', activation='tanh'))\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrGi84fOelXU"
      },
      "source": [
        "#  GAN\n",
        "Time to build a model for **GAN**. We build it by cascading the generator and the discriminator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMUD7Kjejz5G"
      },
      "source": [
        "def buildGan(disc, gen):\n",
        "  model = Sequential()\n",
        "  model.add(gen)\n",
        "  model.add(disc)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eydt1mnBXjZ"
      },
      "source": [
        "disc = buildDisc()\r\n",
        "disc.compile(optimizer=Adam(1e-4), loss=BinaryCrossentropy(True), metrics=['acc'])\r\n",
        "\r\n",
        "gen = buildGen()\r\n",
        "\r\n",
        "disc.trainable = False\r\n",
        "gan = buildGan(disc, gen)\r\n",
        "gan.compile(optimizer=Adam(1e-4), loss=BinaryCrossentropy(True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1urtXngHmIPC"
      },
      "source": [
        "#  Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rzb2V5FHhl0d"
      },
      "source": [
        "batch_size = 128\r\n",
        "for i in range(20000):\r\n",
        "  indexes = np.random.permutation(len(Xtrain))[:batch_size]\r\n",
        "  Xreal = Xtrain[indexes]\r\n",
        "  yreal = np.ones((batch_size,))\r\n",
        "  noise = np.random.randn(batch_size, z_dim)\r\n",
        "  Xfake = gen.predict(noise)\r\n",
        "  yfake = np.zeros_like(yreal)\r\n",
        "  Xdisc = np.concatenate((Xreal, Xfake))\r\n",
        "  ydisc = np.concatenate((yreal, yfake))\r\n",
        "\r\n",
        "  d_loss, d_acc = disc.train_on_batch(Xdisc, ydisc)\r\n",
        "\r\n",
        "\r\n",
        "  noise = np.random.randn(batch_size, z_dim)\r\n",
        "  g_loss = gan.train_on_batch(noise, yreal)\r\n",
        "\r\n",
        "  if not((i+1)%50):\r\n",
        "    print(f'Iteration {i+1}:\\tD acc: {d_acc}\\tD loss: {d_loss}\\tG loss: {g_loss}')\r\n",
        "    noise = np.random.randn(batch_size, z_dim)\r\n",
        "    Xfake = gen.predict(noise)\r\n",
        "    show(Xfake)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJ4bro2SD8sn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}